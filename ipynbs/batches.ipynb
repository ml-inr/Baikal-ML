{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "PROJECT_PATH = \"/home/albert/Baikal-ML/\" #insert your project path\n",
    "sys.path.append(f\"{PROJECT_PATH}\")\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 12:03:19,884 - INFO - Shuffled all file paths. Total paths: 1860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 12:04:28,360 - INFO - #0 chunk loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 58, 5]) torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "from data.root_manager.settings import ChunkGeneratorConfig\n",
    "from data.settings import BatchGeneratorConfig\n",
    "from data.batch_generator import BatchGenerator\n",
    "\n",
    "# Read paths\n",
    "path_mu = \"/net/62/home3/ivkhar/Baikal/data/initial_data/MC_2020/muatm/root/all/\"\n",
    "path_nuatm = \"/net/62/home3/ivkhar/Baikal/data/initial_data/MC_2020/nuatm/root/all/\"\n",
    "path_nu2 = \"/net/62/home3/ivkhar/Baikal/data/initial_data/MC_2020/nue2_100pev/root/all/\"\n",
    "def explore_paths(p: str, start: int, stop: int):\n",
    "    files = os.listdir(p)[start:stop]\n",
    "    return sorted([f\"{p}{file}\" for file in files])\n",
    "mu_paths = explore_paths(path_mu, 0, 800)\n",
    "nuatm_paths = explore_paths(path_nuatm, 0, 1000)\n",
    "nu2_paths = explore_paths(path_nu2, 0, 60)\n",
    "\n",
    "all_paths = mu_paths + nuatm_paths + nu2_paths\n",
    "\n",
    "cfg = BatchGeneratorConfig(\n",
    "    chunk_generator_cfg=ChunkGeneratorConfig(\n",
    "        chunk_size=50,\n",
    "        shuffle_paths = True\n",
    "        ),\n",
    "    batch_size=256, \n",
    "    do_norm=True,\n",
    "    do_augment=True,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "batch_gen = BatchGenerator(root_paths=all_paths, cfg=cfg)\n",
    "batches = batch_gen.get_batches()\n",
    "for b in batches:\n",
    "    input, target = b\n",
    "    print(input.shape, target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_generator_cfg': {'chunk_size': 50,\n",
       "  'processor_params': {'center_times': True,\n",
       "   'calc_tres': False,\n",
       "   'filter_cfg': {'only_signal': True,\n",
       "    'min_hits': 5,\n",
       "    'min_strings': 2,\n",
       "    'min_Q': 0,\n",
       "    't_threshold': 100000.0}},\n",
       "  'fields': ['PulsesAmpl', 'PulsesTime', 'Xrel', 'Yrel', 'Zrel', 'nu_induced'],\n",
       "  'shuffle_paths': True},\n",
       " 'batch_size': 256,\n",
       " 'features_name': ['PulsesAmpl', 'PulsesTime', 'Xrel', 'Yrel', 'Zrel'],\n",
       " 'labels_name': ['nu_induced'],\n",
       " 'do_norm': True,\n",
       " 'norm_params': {'PulsesTime': (0.0, 238.5),\n",
       "  'PulsesAmpl': (6.8, 118.7),\n",
       "  'Xrel': (0.0, 60.0),\n",
       "  'Yrel': (0.0, 60.0),\n",
       "  'Zrel': (0.0, 260.0)},\n",
       " 'do_augment': False,\n",
       " 'augment_parmas': {'PulsesTime': 5.0,\n",
       "  'PulsesAmpl': 0.1,\n",
       "  'Xrel': 2.0,\n",
       "  'Yrel': 2.0,\n",
       "  'Zrel': 5.0},\n",
       " 'shuffle': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average time per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 11:26:13,350 - INFO - #0 chunk loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, torch.Size([256, 50, 5]), torch.Size([256, 1])\n",
      "100, torch.Size([256, 64, 5]), torch.Size([256, 1])\n",
      "200, torch.Size([256, 47, 5]), torch.Size([256, 1])\n",
      "300, torch.Size([256, 59, 5]), torch.Size([256, 1])\n",
      "400, torch.Size([256, 40, 5]), torch.Size([256, 1])\n",
      "500, torch.Size([256, 50, 5]), torch.Size([256, 1])\n",
      "600, torch.Size([256, 65, 5]), torch.Size([256, 1])\n",
      "700, torch.Size([256, 42, 5]), torch.Size([256, 1])\n",
      "800, torch.Size([256, 39, 5]), torch.Size([256, 1])\n",
      "900, torch.Size([256, 37, 5]), torch.Size([256, 1])\n",
      "1000, torch.Size([256, 63, 5]), torch.Size([256, 1])\n",
      "1100, torch.Size([256, 42, 5]), torch.Size([256, 1])\n",
      "1200, torch.Size([256, 33, 5]), torch.Size([256, 1])\n",
      "1300, torch.Size([256, 51, 5]), torch.Size([256, 1])\n",
      "1400, torch.Size([256, 34, 5]), torch.Size([256, 1])\n",
      "1500, torch.Size([256, 73, 5]), torch.Size([256, 1])\n",
      "1600, torch.Size([256, 59, 5]), torch.Size([256, 1])\n",
      "1700, torch.Size([256, 46, 5]), torch.Size([256, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 11:28:05,297 - INFO - #1 chunk loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800, torch.Size([256, 50, 5]), torch.Size([256, 1])\n",
      "1900, torch.Size([256, 37, 5]), torch.Size([256, 1])\n",
      "2000, torch.Size([256, 43, 5]), torch.Size([256, 1])\n",
      "2100, torch.Size([256, 73, 5]), torch.Size([256, 1])\n",
      "2200, torch.Size([256, 60, 5]), torch.Size([256, 1])\n",
      "2300, torch.Size([256, 62, 5]), torch.Size([256, 1])\n",
      "2400, torch.Size([256, 84, 5]), torch.Size([256, 1])\n",
      "2500, torch.Size([256, 51, 5]), torch.Size([256, 1])\n",
      "2600, torch.Size([256, 47, 5]), torch.Size([256, 1])\n",
      "2700, torch.Size([256, 51, 5]), torch.Size([256, 1])\n",
      "2800, torch.Size([256, 48, 5]), torch.Size([256, 1])\n",
      "2900, torch.Size([256, 63, 5]), torch.Size([256, 1])\n",
      "3000, torch.Size([256, 62, 5]), torch.Size([256, 1])\n",
      "3100, torch.Size([256, 43, 5]), torch.Size([256, 1])\n",
      "3200, torch.Size([256, 87, 5]), torch.Size([256, 1])\n",
      "3300, torch.Size([256, 41, 5]), torch.Size([256, 1])\n",
      "3400, torch.Size([256, 45, 5]), torch.Size([256, 1])\n",
      "3500, torch.Size([256, 46, 5]), torch.Size([256, 1])\n",
      "3600, torch.Size([256, 72, 5]), torch.Size([256, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albert/Baikal-ML/venv/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "2024-10-22 11:29:54,187 - INFO - #2 chunk loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700, torch.Size([256, 42, 5]), torch.Size([256, 1])\n",
      "3800, torch.Size([256, 88, 5]), torch.Size([256, 1])\n",
      "3900, torch.Size([256, 42, 5]), torch.Size([256, 1])\n",
      "4000, torch.Size([256, 75, 5]), torch.Size([256, 1])\n",
      "4100, torch.Size([256, 126, 5]), torch.Size([256, 1])\n",
      "4200, torch.Size([256, 50, 5]), torch.Size([256, 1])\n",
      "4300, torch.Size([256, 64, 5]), torch.Size([256, 1])\n",
      "4400, torch.Size([256, 67, 5]), torch.Size([256, 1])\n",
      "4500, torch.Size([256, 38, 5]), torch.Size([256, 1])\n",
      "4600, torch.Size([256, 46, 5]), torch.Size([256, 1])\n",
      "4700, torch.Size([256, 92, 5]), torch.Size([256, 1])\n",
      "4800, torch.Size([256, 43, 5]), torch.Size([256, 1])\n",
      "4900, torch.Size([256, 46, 5]), torch.Size([256, 1])\n",
      "5000, torch.Size([256, 40, 5]), torch.Size([256, 1])\n",
      "5100, torch.Size([256, 77, 5]), torch.Size([256, 1])\n",
      "5200, torch.Size([256, 52, 5]), torch.Size([256, 1])\n",
      "5300, torch.Size([256, 89, 5]), torch.Size([256, 1])\n",
      "5400, torch.Size([256, 56, 5]), torch.Size([256, 1])\n",
      "5500, torch.Size([256, 38, 5]), torch.Size([256, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 11:32:00,646 - INFO - #3 chunk loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600, torch.Size([256, 103, 5]), torch.Size([256, 1])\n",
      "5700, torch.Size([256, 73, 5]), torch.Size([256, 1])\n",
      "5800, torch.Size([256, 51, 5]), torch.Size([256, 1])\n",
      "5900, torch.Size([256, 37, 5]), torch.Size([256, 1])\n",
      "6000, torch.Size([256, 46, 5]), torch.Size([256, 1])\n",
      "6100, torch.Size([256, 29, 5]), torch.Size([256, 1])\n",
      "6200, torch.Size([256, 78, 5]), torch.Size([256, 1])\n",
      "6300, torch.Size([256, 60, 5]), torch.Size([256, 1])\n",
      "6400, torch.Size([256, 50, 5]), torch.Size([256, 1])\n",
      "6500, torch.Size([256, 56, 5]), torch.Size([256, 1])\n",
      "6600, torch.Size([256, 56, 5]), torch.Size([256, 1])\n",
      "6700, torch.Size([256, 69, 5]), torch.Size([256, 1])\n",
      "6800, torch.Size([256, 40, 5]), torch.Size([256, 1])\n",
      "6900, torch.Size([256, 85, 5]), torch.Size([256, 1])\n",
      "7000, torch.Size([256, 39, 5]), torch.Size([256, 1])\n",
      "7100, torch.Size([256, 71, 5]), torch.Size([256, 1])\n",
      "7200, torch.Size([256, 83, 5]), torch.Size([256, 1])\n",
      "7300, torch.Size([256, 40, 5]), torch.Size([256, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 11:33:56,055 - INFO - #4 chunk loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7400, torch.Size([256, 52, 5]), torch.Size([256, 1])\n",
      "7500, torch.Size([256, 50, 5]), torch.Size([256, 1])\n",
      "7600, torch.Size([256, 41, 5]), torch.Size([256, 1])\n",
      "7700, torch.Size([256, 51, 5]), torch.Size([256, 1])\n",
      "7800, torch.Size([256, 48, 5]), torch.Size([256, 1])\n",
      "553.21479845047\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "train_data = batch_gen.get_batches()\n",
    "for i, batch in enumerate(train_data):\n",
    "    inputs, targets = batch\n",
    "    if i%100==0:\n",
    "        print(f\"{i}, {inputs.shape}, {targets.shape}\")\n",
    "    if i>2_000_000/256:\n",
    "        break\n",
    "print(time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s per batch: 0.07081149420166015\n"
     ]
    }
   ],
   "source": [
    "print(f\"s per batch: {553.21479845047/(2_000_000/256)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
