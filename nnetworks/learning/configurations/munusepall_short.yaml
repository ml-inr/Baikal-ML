TrainerConfig:

    num_of_epochs: 10

    steps_per_epoch: null

    checkpoint_interval: 1 # num of epochs

    experiment_path: /home/albert/Baikal-ML/experiments/test # to be changed after loading

    log_interval: 500 # num of batches

    optimizer:
        name: Adam
        kwargs:
            lr: 0.0001
            weight_decay: 1.0e-05

    scheduler: null

    loss:
        name: FocalLoss
        kwargs:
            alpha: 0.25
            gamma: 2
