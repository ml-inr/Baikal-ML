TrainerConfig:

    num_of_epochs: 20

    steps_per_epoch: null

    checkpoint_interval: 2 # num of epochs

    experiment_path: /home/albert/Baikal-ML/experiments/test # to be changed after loading

    log_interval: 500 # num of batches

    early_stopping_patience: 5 # num of epochs

    optimizer:
        name: Adam
        kwargs:
            lr: 0.001
            weight_decay: 1.0e-06

    scheduler:
        name: step_lr
        kwargs:
            step_size: 1
            gamma: 0.85

    loss:
        name: FocalLoss
        kwargs:
            alpha: 0.25
            gamma: 2
